{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries.\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading User Agent from enviornment variables.\n",
    "load_dotenv()\n",
    "User_Agent = os.getenv('USER_AGENT') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": User_Agent }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get HTML data from webpage.\n",
    "def getdata(URL):\n",
    "    r = requests.get(URL,headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get all pages from a page.\n",
    "def getpages(soup, URL):\n",
    "    page = soup.find_all('a', {'data-testid':'pagination-page-next-button'})\n",
    "    if page != None:\n",
    "            page = os.getenv(\"BASE_SITE_URL\") + page[len(page)-2][\"href\"] \n",
    "            return page\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all pages for the website.\n",
    "last_page = 0\n",
    "initial_URL = os.getenv(\"INITIAL_URL\") # Getting the initial URL from the environemnt variables. \n",
    "URL = initial_URL\n",
    "pages = [initial_URL]\n",
    "while(True):\n",
    "    soup = getdata(URL)\n",
    "    URL = getpages(soup, URL)\n",
    "    # max_page = int(URL[-1])\n",
    "    # if max_page > last_page:\n",
    "    #     last_page = max_page\n",
    "    # else:\n",
    "    #     break\n",
    "    pages.append(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists of features.\n",
    "title_c = {\"id\":[],\"title\":[]}\n",
    "address_c = {'id':[], 'address':[]}\n",
    "price_c = {\"id\":[],\"price\":[]}\n",
    "space_cleaned = {\"id\":[],\"space_m2\":[]}\n",
    "rooms_c = {\"id\":[],\"rooms\":[]}\n",
    "bathrooms_c = {\"id\":[],\"bathrooms\":[]}\n",
    "id = 0\n",
    "\n",
    "# Grapping the data of the features from all pagees.\n",
    "for page in pages:\n",
    "    soup = getdata(page)\n",
    "    titles = soup.find_all('h2')\n",
    "    prices = soup.find_all('p', {'data-testid':'property-card-price'})\n",
    "    spaces_m2 = soup.find_all('p', {'data-testid' : 'property-card-spec-area'} )\n",
    "    rooms = soup.find_all('p', {'data-testid' : 'property-card-spec-bedroom'} )\n",
    "    bathrooms = soup.find_all('p', {'data-testid' : 'property-card-spec-bathroom'} )\n",
    "    addresses = soup.find_all('div', {'data-testid' : 'property-card-location'} )\n",
    "\n",
    "    # Cleaning the data before adding them to the lists.\n",
    "    for title, price, space, room, bathroom, address in zip(titles, prices, spaces_m2, rooms, bathrooms, addresses):\n",
    "        title_c['id'].append(id) \n",
    "        title_c['title'].append(title.get_text().strip())\n",
    "\n",
    "        address = address.find('p').get_text().strip()\n",
    "        address_c['id'].append(id)\n",
    "        address_c['address'].append(address)\n",
    "\n",
    "        price = price.get_text().strip()[:-4]\n",
    "        price = price.replace(',', '')\n",
    "        price = price.replace('\\n', '')\n",
    "        price = price.replace('Ask for p', '')\n",
    "        if price == '':\n",
    "            price = 0\n",
    "        price_c['id'].append(id) \n",
    "        price_c['price'].append(int(price))\n",
    "\n",
    "        space = space.get_text().strip()\n",
    "        # space = re.sub(\"[^A-Za-z0-9]\",\"\",space)[:-1]\n",
    "        space_cleaned[\"id\"].append(id)\n",
    "        space_cleaned['space_m2'].append(space)\n",
    "\n",
    "        room = room.get_text().strip()\n",
    "        rooms_c['id'].append(id)\n",
    "        rooms_c['rooms'].append(room)\n",
    "\n",
    "        bathroom = bathroom.get_text().strip()\n",
    "        bathrooms_c['id'].append(id)\n",
    "        bathrooms_c['bathrooms'].append(bathroom)\n",
    "        id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the data into a csv file.\n",
    "header = ['id', 'title', 'address', 'price', 'space_sqm', 'rooms', \"bathrooms\"]\n",
    "with open(\"real estate data.csv\", \"a+\", newline='', encoding='UTF-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for id, title, address, price, space, rooms, bathrooms in zip(title_c['id'], title_c['title'], address_c['address'], price_c['price'], space_cleaned['space_m2'], rooms_c['rooms'], bathrooms_c['bathrooms']):\n",
    "        writer.writerow([id, title, address, price, space, rooms, bathrooms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
